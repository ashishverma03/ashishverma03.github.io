<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0, shrink-to-fit=no" />

    <!--Update this for shortest possible description-->
    <meta name="description"
        content="DiViScan: Generative Augmentation Driven Prediction of Diverse Visual Scanpaths in Images." />

    <!--This part is to give credit to the person who developed the site-->
    <meta name="author" content="Aniket Patra" />

    <!---------------------og tags are for open graph ----------------->
    <meta property="og:title" content="Diverse-Visual-Scanpath" />

    <meta property="og:url" content="https://ashishverma03.github.io/diverse-visual-scanpath" />
    <meta property="og:image" content="https://ashishverma03.github.io/assets/images/lfsfa_thumb.jpg" />
    <meta property="og:image:type" content="image/jpg" />
    <meta property="og:image:width" content="1200" />
    <meta property="og:image:height" content="630" />
    <meta property="og:image:alt" content="Iterative Dehazing Training Stage 3" />

    <meta property="og:type" content="article" />
    <meta property="og:description"
        content="DiViScan: Generative Augmentation Driven Prediction of Diverse Visual Scanpaths in Images." />

    <meta property="og:locale" content="en_IN" />
    <meta property="og:locale:alternate" content="en_US" />

    <!---------------TWITTER CRDS------------------------>
    <meta name="twitter:card" content="summary_large_image" />
    <meta name="twitter:title" content="Diverse-Visual-Scanpath" />
    <meta name="twitter:description"
        content="DiViScan: Generative Augmentation Driven Prediction of Diverse Visual Scanpaths in Images." />
    <meta name="twitter:url" content="https://ashishverma03.github.io/diverse-visual-scanpath" />
    <meta name="twitter:image" content="https://ashishverma03.github.io/assets/images/diviscan_thumb.jpg" />

    <title>DiViScan</title>

    <!--Page Icon-->
    <link rel="icon" type="image/png" href="assets/images/ak2.ico" />

    <!--STYLESHEETS (.css)-->

    <!--For Bootstrap 5-->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.0.2/dist/css/bootstrap.min.css" rel="stylesheet"
        integrity="sha384-EVSTQN3/azprG1Anm3QDgpJLIm9Nao0Yz1ztcQTwFspd3yD65VohhpuuCOmLASjC" crossorigin="anonymous">

    <!--Extrenal css-->
    <link rel="stylesheet" href="diverse-visual-scanpath/subpage.css" />
</head>

<body>
    <div class="container" style="background-color: rgb(255, 255, 255)">
        <!--Top Nav-->
        <header>
            <nav class="navbar navbar-expand-lg navbar-dark bg-dark">
                <div class="container-fluid">
                    <a class="navbar-brand" href="https://ashishverma03.github.io/"><i
                            class="fas fa-house-user d-inline-block"></i></a>
                    <button class="navbar-toggler" type="button" data-bs-toggle="collapse"
                        data-bs-target="#navbarSupportedContent" aria-controls="navbarSupportedContent"
                        aria-expanded="false" aria-label="Toggle navigation">
                        <span class="navbar-toggler-icon"></span>
                    </button>
                    <div class="collapse navbar-collapse" id="navbarSupportedContent">
                        <ul class="navbar-nav me-auto mb-2 mb-lg-0">
                            <li class="nav-item">
                                <a class="nav-link" href="#sectionResult">Result</a>
                            </li>
                            <li class="nav-item">
                                <a class="nav-link" href="#download">Download</a>
                            </li>
                            <li class="nav-item">
                                <a class="nav-link" href="#cite">Citation</a>
                            </li>
                            <li class="nav-item">
                                <a class="nav-link" id="mailTo">Contact</a>
                            </li>
                        </ul>
                    </div>
                </div>
            </nav>
        </header>

        <!--Topic Name-->
        <h1 style="
          text-align: center;
          font-size: calc(25px + 0.5vw);
          font-family: 'Raleway', sans-serif;
          font-weight: bold;
        ">
            DiViScan: Generative Augmentation Driven Prediction of Diverse Visual Scanpaths in Images
        </h1>

        <!--Author Name-->

        <h2 style="
          text-align: center;
          font-size: calc(10px + 0.5vw);
          font-family: 'Roboto', sans-serif;
        ">
            <a href="https://github.com/ashishverma03" target="_blank" style="text-decoration: none; color: rgb(70, 70, 70)"
                onmouseover="this.style.color='gray'" onmouseout="this.style.color='rgb(70, 70, 70)'"><u>Ashish
                    Verma</u></a>, Debashis Sen
        </h2>

        <!--Institution Name-->

        <h3 style="
          text-align: center;
          font-size: calc(10px + 0.5vw);
          font-family: 'Roboto', sans-serif;
        ">
            Department of Electronics and Electrical Communication Engineering
            <br />
            Indian Institute of Technology Kharagpur, India
        </h3>
        <hr />
        <!-------------------------------FIGURE 1--------------------------->
        <div class="row ">
            <div class="col d-flex justify-content-center">
                <figure class="figure">
                    <span class="d-flex justify-content-center">
                        <img class="img-fluid figure-img" src="assets/subpage-diverse-visual-scanpath/teaser/fig1.jpg"
                            class="figure-img img-fluid rounded" title="Image Training" height="800" width="1200"
                            alt="machine learning image training diagram" /></a></span>
                    <figcaption class="text-center" id="figCap">
                        The architecture of the proposed model for visual scanpath 
                        prediction comprises of two main novel components: augmentation
                        of image-visual scanpath pairs using HMM and the LSTM-based 
                        scanpath predictor.
                        <!--EDIT CAPTION HERE-->
                    </figcaption>
                </figure>
            </div>
        </div>
        <hr>
        <!------------------------------ABSTRACT-------------------->
        <h2 id="headingStyles" class="text-center text-md-start">
            <i class="fas fa-lightbulb"></i>&nbsp;Abstract
        </h2>
        <div class="row">
            <div class="col-sm">
                <p style="text-align: justify">
                    Multiple visual scanpaths in an image represent the process by
                    which different humans capture the information in it. A deep 
                    network for predicting such multiple diverse visual scanpaths 
                    is proposed in this paper. Image-specific hidden Markov model 
                    based generative data augmentation is performed in the beginning 
                    to increase the number of image-visual scanpath training pairs. 
                    Showing that our generative data augmentation is well-suited for 
                    long short-term memory (LSTM) based prediction, an LSTM based visual 
                    scanpath predictor is proposed. A network to predict a single visual 
                    scanpath on an image is designed first. The network is then modified 
                    to predict multiple diverse scanpaths representing different 
                    viewer varieties by using a parameter indicating the uniqueness 
                    of a viewer and a random vector for subtle variations. Our models 
                    are evaluated on three standard datasets using multiple performance 
                    measures, which demonstrate the superiority of the proposed approach 
                    over the state-of-the-art. Empirical studies are also given indicating 
                    the significance of our novel generative data augmentation and multiple 
                    scanpath generation methods.
                </p>
            </div>
        </div>
        <hr>
        <!------------------------------HIGHLIGHTS-------------------->
        <h2 id="headingStyles" class="text-center text-md-start">
            <i class="fas fa-satellite-dish"></i>&nbsp;Highlights
        </h2>
        <div class="row" id="jumbo">
            <div class="col">
                <ol type="i">
                    <li>
                        An LSTM-based visual scanpath prediction network, 
                        which is trained end-to-end without involving any 
                        predefined model.
                    </li>
                    <li>
                        The training is based on the uniqueness of a viewer 
                        to generate multiple and distinct visual scanpaths 
                        for an image.
                    </li>
                    <li>
                        A HMM-based generative data augmentation procedure 
                        to obtain image-specific training pairs of images 
                        & visual scanpaths.
                    </li>
                </ol>
            </div>
        </div>
        <hr />
        <!------------------------------Proposed module-------------------->
        <h2 id="headingStyles" class="text-center text-md-start">
            <i class="fas fa-puzzle-piece"></i>&nbsp;Proposed Module
        </h2>
        <div class="row ">
            <!-------------------------------FIGURE 2--------------------------->
            <div class="col d-flex justify-content-center">
                <figure class="figure">
                    <span class="d-flex justify-content-center">
                        <img class="img-fluid figure-img" src="assets/lfsafa-sr/teaser/fig2.jpg"
                            class="figure-img img-fluid rounded" title="Proposed sub-aperture feature adaptation module"
                            height="600" width="600"
                            alt="proposed sub-aperture feature adaptation module image" /></a></span>
                    <figcaption class="text-center" id="figCap">
                        The proposed sub-aperture feature adaptation module
                        consists of <i>n</i> SAS modules and 1 fusion module. <i>f</i><sub>i</sub> is the
                        extracted feature of a sub-aperture image using a pre-trained
                        model <i>F</i><sub>feat</sub> and <i>f'</i><sub>i</sub> is the modulated feature that contains
                        more rich features that are acquired from other sub-aperture
                        images. '<i>Conv, a, b, k</i>' represents 2D convolution with <i>a</i>
                        number of input channels, <i>b</i> number of output channels, and
                        <i>k</i> is the kernel size.
                        ule <i>F</i><sub>feat</sub>, and another one is upscaling cum reconstruction
                        module <i>F</i><sub>up</sub>. <i>F</i><sub>feat</sub> extracts the salient features from a
                        single
                        image that is up-scaled by <i>F</i><sub>up</sub>. Our main objective is to in-
                        troduce a module that modulates the extracted features from
                        <i>F</i><sub>feat</sub> by exploiting angular information across SAIs.
                        <!--EDIT CAPTION HERE-->
                    </figcaption>
                </figure>
            </div>
        </div>
        <hr>
        <!------------------------------Results and ABalation Studies-------------------->
        <div class="row" id="sectionResult">
            <!-------------------------------FIGURE 2--------------------------->
            <div class="col-sm-6">
                <h2 id="headingStyles" class="text-center text-md-start">
                    <i class="fas fa-poll"></i>&nbsp;Results
                </h2>
                <figure class="figure">
                    <span class="d-flex justify-content-center">
                        <img class="img-fluid figure-img" src="assets/lfsafa-sr/resultAbalation/tab1.jpg"
                            class="figure-img img-fluid rounded" title="Proposed sub-aperture feature adaptation module"
                            height="600" width="600"
                            alt="proposed sub-aperture feature adaptation module image" /></a></span>
                    <figcaption class="text-center" id="figCap">
                        Table 1: PSNR/SSIM values achieved by different methods
                        for 2x and 4xSR. Our results are shown in bold
                        <!--EDIT CAPTION HERE-->
                    </figcaption>
                </figure>
            </div>

            <div class="col-sm-6">
                <h2 id="headingStyles" class="text-center text-md-start">
                    <i class="fas fa-poll"></i>&nbsp;Abalation Study
                </h2>
                <div class="row">
                    <div class="col-12">
                        <figure class="figure">
                            <span class="d-flex justify-content-center">
                                <img class="img-fluid figure-img" src="assets/lfsafa-sr/resultAbalation/tab2.jpg"
                                    class="figure-img img-fluid rounded" title="Model ablation studies" height="600"
                                    width="600" alt="Table number 2, caption is mentioned below." /></a></span>
                            <figcaption class="text-center" id="figCap">
                                Table 2: Model ablation studies of our proposed LFSAFA
                                module and the effect of angular resolution on the reconstruction performance. All the
                                experiments are performed on the
                                LFSAFA-RDN variant for 2x SR.
                                <!--EDIT CAPTION HERE-->
                            </figcaption>
                        </figure>
                    </div>
                    <br>
                    <div class="col-12">
                        <figure class="figure">
                            <span class="d-flex justify-content-center">
                                <img class="img-fluid figure-img" src="assets/lfsafa-sr/resultAbalation/tab3.jpg"
                                    class="figure-img img-fluid rounded" title="Comparative analysis" height="600"
                                    width="600" alt="Table number 3, caption is mentioned below." /></a></span>
                            <figcaption class="text-center" id="figCap">
                                Table 3: Comparative analysis of our proposed LFSAFA
                                module-based LFSR models with their SISR counterparts.
                                <!--EDIT CAPTION HERE-->
                            </figcaption>
                        </figure>
                    </div>
                </div>
            </div>
        </div>
        <hr>
        <!------------------------------Visual Comparison-------------------->
        <h2 id="headingStyles" class="text-center text-md-start">
            <i class="fas fa-eye"></i>&nbsp;Visual Comparison
        </h2>
        <div class="row ">
            <!-------------------------------FIGURE 2--------------------------->
            <div class="col d-flex justify-content-center">
                <figure class="figure">
                    <span class="d-flex justify-content-center">
                        <img class="img-fluid figure-img" src="assets/lfsafa-sr/visComparison/fig3.jpg"
                            class="figure-img img-fluid rounded"
                            title="Qualitative comparison of our proposed LFSAFA-RDN" width="1300"
                            alt="Image for qualitative comparison of our proposed LFSAFA-RDN" /></a></span>
                    <figcaption class="text-center" id="figCap">
                        Qualitative comparison of our proposed LFSAFA-RDN with the existing LFSR algorithms for 4x SR
                        <!--EDIT CAPTION HERE-->
                    </figcaption>
                </figure>
            </div>
        </div>
        <hr>
        <!-----------------------DOWNLOADS------------------------------->
        <h2 id="download" class="text-center text-md-start">
            <i class="fas fa-download"></i>&nbsp;Download
        </h2>
        <div class="row">
            <div class="col-sm-4 text-center">
                <figure class="figure">
                    <a href="#########ENTER_LINK###########" rel="external nofollow" target="_blank"
                        onmouseover="this.style.opacity='0.5'" onmouseout="this.style.opacity='1'"><img
                            src="assets/images/arxiv.png" alt="Arxiv image icon" width="220px" height="120px"
                            title="Click to Open" />
                    </a>
                    <figcaption class="text-center figure-caption" style="font-size: calc(12px + 0.5vw)">
                        Paper
                    </figcaption>
                </figure>
            </div>
            <div class="col-sm-4 text-center">
                <figure class="figure">
                    <a href="https://github.com/aupendu/LFSAFA-SR" rel="external nofollow" target="_blank"
                        onmouseover="this.style.opacity='0.5'" onmouseout="this.style.opacity='1'"><i
                            class="fab fa-github" style="font-size: 120px; color: rgb(54, 54, 54)"></i>
                    </a>
                    <figcaption class="text-center figure-caption" style="font-size: calc(12px + 0.5vw)">
                        Code
                    </figcaption>
                </figure>
            </div>
            <div class="col-sm-4 text-center">
                <figure class="figure">
                    <a href="#########ENTER_LINK###########" rel="external nofollow" target="_blank"
                        onmouseover="this.style.opacity='0.5'" onmouseout="this.style.opacity='1'"><img
                            src="assets/images/gdrive.png" alt="Google drive image icon" width="150px" height="120px"
                            title="Click to Open" />
                    </a>
                    <figcaption class="text-center figure-caption" style="font-size: calc(12px + 0.5vw)">
                        Training & Testing Datasets
                    </figcaption>
                </figure>
            </div>
        </div>
        <hr />
        <!-------------------------REFERENCES---------------------------->
        <h2 id="headingStyles" class="text-center text-md-start">
            <i class="fas fa-asterisk"></i>&nbsp;References
        </h2>
        <div class="row">
            <div class="col">
                <ul>
                    <li style="text-align: justify">
                        [7] Jiwon Kim, Jung Kwon Lee, and Kyoung Mu Lee, “Accurate image super-resolution using very deep convolutional networks,” in CVPR, 2016, pp. 1646–1654.
                    </li>
                    <li style="text-align: justify">
                        [10] Yulun Zhang, Kunpeng Li, et al., “Image superresolution using very deep residual channel attention networks,” in ECCV, 2018, pp. 286–301
                    </li>
                    <li style="text-align: justify">
                        [11] Shuo Zhang, Youfang Lin, and Hao Sheng, “Residual networks for light field image super-resolution,” in CVPR, 2019, pp. 11046–11055.
                    </li>
                    <li style="text-align: justify">
                        [12] Henry Wing Fung Yeung, et al., “Light field spatial super-resolution using deep efficient spatialangular separable convolution,” IEEE TIP, vol. 28, no. 5, pp. 2319–2330, 2018.
                    </li>
                    <li style="text-align: justify">
                        [13] Yingqian Wang, Longguang Wang, et al., “Spatialangular interaction for light field image superresolution,” in ECCV, 2020.
                    </li>
                    <li style="text-align: justify">
                        [14] Yingqian Wang, Jungang Yang, et al., “Light field image super-resolution using deformable convolution,” IEEE TIP, vol. 30, pp. 1057–1071, 2020.
                    </li>
                    <li style="text-align: justify">
                        [20] Shuo Zhang, Song Chang, and Youfang Lin, “Endto-end light field spatial super-resolution network using multiple epipolar geometry,” IEEE TIP, vol. 30, pp. 5956–5968, 2021.
                    </li>
                    <li style="text-align: justify">
                        [21] Shunzhou Wang, Tianfei Zhou, Yao Lu, and Huijun Di, “Detail-preserving transformer for light field image super-resolution,” in AAAI, 2022.
                    </li>
                </ul>
            </div>
        </div>
        <hr />
        <!-------------------------CITATION---------------------------->
        <h2 id="cite" class="text-center text-md-start">
            <i class="fas fa-map-marker"></i>&nbsp;Citation (BibTeX)
        </h2>
        <div class="row">
            <div class="col" id="jumbo">
        <p id="myInput">
          <span style="display: block; padding-left: 35px">@misc{kar2022subaperture,><br />
            <span style="padding-left: 53px; display: block">title={Sub-Aperture Feature Adaptation in Single Image 
                Super-resolution Model for Light Field Imaging},<br />
              author={Aupendu Kar and Suresh Nehra and Jayanta Mukhopadhyay and
              Prabir Kumar Biswas},<br />
              booktitle={2022 IEEE International Conference on Image Processing (ICIP)},<br />
              year={2022},<br />
              organization={IEEE}<br /></span>
            }</span>
        </p>
            </div>
        </div>
        <hr />

        <!-----------------------------FOOTER------------------------------->
        <footer>
            <p style="text-align: center">
                <i class="far fa-copyright"></i> 2022
                <a href="https://github.com/aupendu" target="_blank">Aupendu Kar</a>.
                Made in
                <a href="https://www.incredibleindia.org/" target="_blank"><img id="flag"
                        src="/assets/images/india-flag-icon-32.png" alt="My Great Country India's Flag" /></a>
                by
                <a href="https://thingsbypatra.pythonanywhere.com//" target="_blank">Patra
                </a>
            </p>
        </footer>
    </div>

    <!--SCRIPTS (.js)-->
    <!--JQUERY-->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.6.0/jquery.min.js"
        integrity="sha512-894YE6QWD5I59HgZOGReFYm4dnWc1Qt5NtvYSaNcOP+u1T9qYdvdihz0PPSiiqn/+/3e7Jo4EaG7TubfWGUrMQ=="
        crossorigin="anonymous" referrerpolicy="no-referrer"></script>

    <!--Font Awesome 5-->
    <script src="https://kit.fontawesome.com/58bac38d13.js" crossorigin="anonymous"></script>

    <!--Bootstrap 5-->
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.0.2/dist/js/bootstrap.bundle.min.js"
        integrity="sha384-MrcW6ZMFYlzcLA8Nl+NtUVF0sA7MsXsP1UyJoMp4YLEuNSfAP+JcXn/tWtIaxVXM"
        crossorigin="anonymous"></script>

    <!--External JS-->
    <script src="lfsafa-sr/subjs.js"></script>
</body>

</html>
